{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "import os, sys\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, \\\n",
    "  Bidirectional, RepeatVector, Concatenate, Activation, Dot, Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "  from keras.layers import CuDNNLSTM as LSTM\n",
    "  from keras.layers import CuDNNGRU as GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "  assert(K.ndim(x) > 2)\n",
    "  e = K.exp(x - K.max(x, axis=1, keepdims=True))\n",
    "  s = K.sum(e, axis=1, keepdims=True)\n",
    "  return e / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 256\n",
    "LATENT_DIM_DECODER = 256 # idea: make it different to ensure things all fit together properly!\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where we will store the data\n",
    "input_texts = [] # sentence in original language\n",
    "target_texts = [] # sentence in target language\n",
    "target_texts_inputs = [] # sentence in target language offset by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 10000\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for line in open('DATASET/spa-eng/spa.txt'):\n",
    "  # only keep a limited number of samples\n",
    "  t += 1\n",
    "  if t > NUM_SAMPLES:\n",
    "    break\n",
    "\n",
    "  # input and target are separated by tab\n",
    "  if '\\t' not in line:\n",
    "    continue\n",
    "\n",
    "  # split up the input and translation\n",
    "  input_text, translation = line.rstrip().split('\\t')\n",
    "\n",
    "  # make the target input and output\n",
    "  # recall we'll be using teacher forcing\n",
    "  target_text = translation + ' <eos>'\n",
    "  target_text_input = '<sos> ' + translation\n",
    "\n",
    "  input_texts.append(input_text)\n",
    "  target_texts.append(target_text)\n",
    "  target_texts_inputs.append(target_text_input)\n",
    "print(\"num samples:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2368 unique input tokens.\n"
     ]
    }
   ],
   "source": [
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the outputs\n",
    "# don't filter out special characters\n",
    "# otherwise <sos> and <eos> won't appear\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6302 unique output tokens.\n"
     ]
    }
   ],
   "source": [
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store number of output words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_output = len(word2idx_outputs) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_data.shape: (10000, 5)\n",
      "encoder_data[0]: [ 0  0  0  0 14]\n",
      "decoder_data[0]: [   2 1481    0    0    0    0    0    0    0]\n",
      "decoder_data.shape: (10000, 9)\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_data.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_data[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_data.shape:\", decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# store all the pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('DATASET/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "  if i < MAX_NUM_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    "  # trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create targets, since we cannot use sparse\n",
    "# categorical cross entropy when we have sequences\n",
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the values\n",
    "for i, d in enumerate(decoder_targets):\n",
    "  for t, word in enumerate(d):\n",
    "    decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### build the model #####\n",
    "\n",
    "# Set up the encoder - simple!\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_sequences=True,\n",
    "  # dropout=0.5 # dropout not available on gpu\n",
    "))\n",
    "encoder_outputs = encoder(x)\n",
    "\n",
    "\n",
    "# Set up the decoder - not so simple\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "\n",
    "# this word embedding will not use pre-trained vectors\n",
    "# although you could\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Attention #########\n",
    "# Attention layers need to be global because\n",
    "# they will be repeated Ty times at the decoder\n",
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis=-1)\n",
    "attn_dense1 = Dense(10, activation='tanh')\n",
    "attn_dense2 = Dense(1, activation=softmax_over_time)\n",
    "attn_dot = Dot(axes=1) # to perform the weighted sum of alpha[t] * h[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(h, st_1):\n",
    "  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
    "  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
    " \n",
    "  # copy s(t-1) Tx times\n",
    "  # now shape = (Tx, LATENT_DIM_DECODER)\n",
    "  st_1 = attn_repeat_layer(st_1)\n",
    "\n",
    "  # Concatenate all h(t)'s with s(t-1)\n",
    "  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
    "  x = attn_concat_layer([h, st_1])\n",
    "\n",
    "  # Neural net first layer\n",
    "  x = attn_dense1(x)\n",
    "\n",
    "  # Neural net second layer with special softmax over time\n",
    "  alphas = attn_dense2(x)\n",
    "\n",
    "  # \"Dot\" the alphas and the h's\n",
    "  # Remember a.dot(b) = sum over a[t] * b[t]\n",
    "  context = attn_dot([alphas, h])\n",
    "\n",
    "  return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the rest of the decoder (after attention)\n",
    "decoder_lstm = LSTM(LATENT_DIM_DECODER, return_state=True)\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "\n",
    "initial_s = Input(shape=(LATENT_DIM_DECODER,), name='s0')\n",
    "initial_c = Input(shape=(LATENT_DIM_DECODER,), name='c0')\n",
    "context_last_word_concat_layer = Concatenate(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s, c will be re-assigned in each iteration of the loop\n",
    "s = initial_s\n",
    "c = initial_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect outputs in a list at first\n",
    "outputs = []\n",
    "for t in range(max_len_target): # Ty times\n",
    "  # get the context using attention\n",
    "  context = one_step_attention(encoder_outputs, s)\n",
    "\n",
    "  # we need a different layer for each time step\n",
    "  selector = Lambda(lambda x: x[:, t:t+1])\n",
    "  xt = selector(decoder_inputs_x)\n",
    "  \n",
    "  # combine \n",
    "  decoder_lstm_input = context_last_word_concat_layer([context, xt])\n",
    "\n",
    "  # pass the combined [context, last word] into the LSTM\n",
    "  # along with [s, c]\n",
    "  # get the new [s, c] and output\n",
    "  o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[s, c])\n",
    "\n",
    "  # final dense layer to get next word prediction\n",
    "  decoder_outputs = decoder_dense(o)\n",
    "  outputs.append(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_and_transpose(x):\n",
    "  # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "  x = K.stack(x) # is now T x batch_size x output_vocab_size tensor\n",
    "  x = K.permute_dimensions(x, pattern=(1, 0, 2)) # is now batch_size x T x output_vocab_size\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it a layer\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model = Model(\n",
    "  inputs=[\n",
    "    encoder_inputs_placeholder,\n",
    "    decoder_inputs_placeholder,\n",
    "    initial_s, \n",
    "    initial_c,\n",
    "  ],\n",
    "  outputs=outputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 66s 8ms/step - loss: 2.7366 - acc: 0.6326 - val_loss: 2.6753 - val_acc: 0.6492\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 2.0574 - acc: 0.7109 - val_loss: 2.4330 - val_acc: 0.6731\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 1.8343 - acc: 0.7316 - val_loss: 2.2753 - val_acc: 0.6892\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 1.6687 - acc: 0.7509 - val_loss: 2.1584 - val_acc: 0.7052\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 1.5337 - acc: 0.7660 - val_loss: 2.0700 - val_acc: 0.7147\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 1.4231 - acc: 0.7790 - val_loss: 2.0247 - val_acc: 0.7221\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 1.3252 - acc: 0.7917 - val_loss: 1.9786 - val_acc: 0.7292\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 1.2405 - acc: 0.8026 - val_loss: 1.9704 - val_acc: 0.7362\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 1.1685 - acc: 0.8125 - val_loss: 1.9293 - val_acc: 0.7394\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 1.1002 - acc: 0.8204 - val_loss: 1.9048 - val_acc: 0.7421\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 1.0380 - acc: 0.8298 - val_loss: 1.8917 - val_acc: 0.7444\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.9847 - acc: 0.8365 - val_loss: 1.8854 - val_acc: 0.7446\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.9362 - acc: 0.8436 - val_loss: 1.9002 - val_acc: 0.7454\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.8894 - acc: 0.8500 - val_loss: 1.9024 - val_acc: 0.7428\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.8437 - acc: 0.8575 - val_loss: 1.9221 - val_acc: 0.7398\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.8027 - acc: 0.8633 - val_loss: 1.9137 - val_acc: 0.7439\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.7618 - acc: 0.8698 - val_loss: 1.9039 - val_acc: 0.7471\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.7246 - acc: 0.8747 - val_loss: 1.9141 - val_acc: 0.7422\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.6945 - acc: 0.8803 - val_loss: 1.9264 - val_acc: 0.7394\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.6675 - acc: 0.8846 - val_loss: 1.9289 - val_acc: 0.7426\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.6398 - acc: 0.8883 - val_loss: 1.9461 - val_acc: 0.7407\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.6152 - acc: 0.8923 - val_loss: 1.9641 - val_acc: 0.7379\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.5957 - acc: 0.8970 - val_loss: 1.9653 - val_acc: 0.7405\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.5767 - acc: 0.8995 - val_loss: 1.9900 - val_acc: 0.7366\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.5561 - acc: 0.9026 - val_loss: 1.9818 - val_acc: 0.7382\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.5414 - acc: 0.9047 - val_loss: 2.0059 - val_acc: 0.7361\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.5271 - acc: 0.9079 - val_loss: 2.0339 - val_acc: 0.7331\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.5101 - acc: 0.9095 - val_loss: 2.0396 - val_acc: 0.7331\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.4917 - acc: 0.9128 - val_loss: 2.0341 - val_acc: 0.7347\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.4762 - acc: 0.9156 - val_loss: 2.0575 - val_acc: 0.7303\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.4634 - acc: 0.9173 - val_loss: 2.0563 - val_acc: 0.7312\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.4526 - acc: 0.9195 - val_loss: 2.0626 - val_acc: 0.7326\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.4430 - acc: 0.9218 - val_loss: 2.0554 - val_acc: 0.7342\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.4321 - acc: 0.9226 - val_loss: 2.0856 - val_acc: 0.7296\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.4215 - acc: 0.9243 - val_loss: 2.0939 - val_acc: 0.7289\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.4126 - acc: 0.9264 - val_loss: 2.0976 - val_acc: 0.7312\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.4052 - acc: 0.9277 - val_loss: 2.1090 - val_acc: 0.7308\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.3987 - acc: 0.9286 - val_loss: 2.1262 - val_acc: 0.7288\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3923 - acc: 0.9285 - val_loss: 2.1410 - val_acc: 0.7288\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.3849 - acc: 0.9315 - val_loss: 2.1340 - val_acc: 0.7270\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3770 - acc: 0.9318 - val_loss: 2.1407 - val_acc: 0.7277\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3694 - acc: 0.9335 - val_loss: 2.1515 - val_acc: 0.7272\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3610 - acc: 0.9345 - val_loss: 2.1438 - val_acc: 0.7279\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3530 - acc: 0.9360 - val_loss: 2.1574 - val_acc: 0.7272\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.3456 - acc: 0.9368 - val_loss: 2.1533 - val_acc: 0.7287\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.3395 - acc: 0.9381 - val_loss: 2.1647 - val_acc: 0.7264\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3356 - acc: 0.9387 - val_loss: 2.1809 - val_acc: 0.7274\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3327 - acc: 0.9392 - val_loss: 2.1723 - val_acc: 0.7279\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.3294 - acc: 0.9399 - val_loss: 2.1783 - val_acc: 0.7283\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.3274 - acc: 0.9403 - val_loss: 2.1896 - val_acc: 0.7257\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.3243 - acc: 0.9407 - val_loss: 2.2032 - val_acc: 0.7271\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.3226 - acc: 0.9407 - val_loss: 2.2074 - val_acc: 0.7260\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3200 - acc: 0.9410 - val_loss: 2.2216 - val_acc: 0.7234\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3181 - acc: 0.9408 - val_loss: 2.2158 - val_acc: 0.7236\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3153 - acc: 0.9417 - val_loss: 2.2314 - val_acc: 0.7253\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3130 - acc: 0.9420 - val_loss: 2.2167 - val_acc: 0.7273\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.3109 - acc: 0.9419 - val_loss: 2.2336 - val_acc: 0.7251\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.3085 - acc: 0.9424 - val_loss: 2.2319 - val_acc: 0.7258\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3076 - acc: 0.9433 - val_loss: 2.2355 - val_acc: 0.7247\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3052 - acc: 0.9432 - val_loss: 2.2506 - val_acc: 0.7241\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3037 - acc: 0.9436 - val_loss: 2.2503 - val_acc: 0.7249\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3009 - acc: 0.9437 - val_loss: 2.2598 - val_acc: 0.7253\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2991 - acc: 0.9441 - val_loss: 2.2644 - val_acc: 0.7246\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2981 - acc: 0.9438 - val_loss: 2.2860 - val_acc: 0.7237\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.2930 - acc: 0.9448 - val_loss: 2.2749 - val_acc: 0.7255\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.2895 - acc: 0.9450 - val_loss: 2.2924 - val_acc: 0.7248\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2869 - acc: 0.9462 - val_loss: 2.2852 - val_acc: 0.7252\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.2847 - acc: 0.9468 - val_loss: 2.2788 - val_acc: 0.7242\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2842 - acc: 0.9471 - val_loss: 2.2848 - val_acc: 0.7242\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2829 - acc: 0.9469 - val_loss: 2.2969 - val_acc: 0.7233\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2822 - acc: 0.9469 - val_loss: 2.2936 - val_acc: 0.7241\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 37s 5ms/step - loss: 0.2814 - acc: 0.9473 - val_loss: 2.3039 - val_acc: 0.7244\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2812 - acc: 0.9470 - val_loss: 2.3053 - val_acc: 0.7239\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2807 - acc: 0.9474 - val_loss: 2.2988 - val_acc: 0.7252\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2795 - acc: 0.9473 - val_loss: 2.3226 - val_acc: 0.7231\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2793 - acc: 0.9478 - val_loss: 2.3191 - val_acc: 0.7243\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2790 - acc: 0.9483 - val_loss: 2.3197 - val_acc: 0.7243\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2800 - acc: 0.9478 - val_loss: 2.3142 - val_acc: 0.7247\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2807 - acc: 0.9480 - val_loss: 2.3361 - val_acc: 0.7234\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2809 - acc: 0.9474 - val_loss: 2.3195 - val_acc: 0.7247\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2814 - acc: 0.9474 - val_loss: 2.3311 - val_acc: 0.7246\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2812 - acc: 0.9471 - val_loss: 2.3328 - val_acc: 0.7262\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2811 - acc: 0.9479 - val_loss: 2.3393 - val_acc: 0.7243\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2817 - acc: 0.9473 - val_loss: 2.3483 - val_acc: 0.7242\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2817 - acc: 0.9476 - val_loss: 2.3450 - val_acc: 0.7221\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2810 - acc: 0.9473 - val_loss: 2.3428 - val_acc: 0.7249\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2812 - acc: 0.9479 - val_loss: 2.3521 - val_acc: 0.7256\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2815 - acc: 0.9477 - val_loss: 2.3513 - val_acc: 0.7242\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2819 - acc: 0.9479 - val_loss: 2.3518 - val_acc: 0.7240\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2818 - acc: 0.9478 - val_loss: 2.3584 - val_acc: 0.7251\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2822 - acc: 0.9472 - val_loss: 2.3623 - val_acc: 0.7243\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.2829 - acc: 0.9473 - val_loss: 2.3716 - val_acc: 0.7244\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2835 - acc: 0.9477 - val_loss: 2.3692 - val_acc: 0.7224\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.2831 - acc: 0.9477 - val_loss: 2.3812 - val_acc: 0.7245\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2838 - acc: 0.9473 - val_loss: 2.3719 - val_acc: 0.7236\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.2846 - acc: 0.9477 - val_loss: 2.3824 - val_acc: 0.7231\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2853 - acc: 0.9478 - val_loss: 2.3856 - val_acc: 0.7238\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2859 - acc: 0.9473 - val_loss: 2.3892 - val_acc: 0.7235\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.2867 - acc: 0.9474 - val_loss: 2.3966 - val_acc: 0.7226\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.2870 - acc: 0.9468 - val_loss: 2.3871 - val_acc: 0.7238\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "z = np.zeros((NUM_SAMPLES, LATENT_DIM_DECODER)) # initial [s, c]\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs, z, z], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a63743780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4yJI/8fiumy9r2bQ9IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1a637d87f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The encoder will be stand-alone\n",
    "# From this we will get our initial decoder hidden state\n",
    "# i.e. h(1), ..., h(Tx)\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
    "\n",
    "# next we define a T=1 decoder model\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# no need to loop over attention steps this time because there is only one step\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
    "\n",
    "# combine context with last word\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "# lstm and final dense\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "  # Encode the input as state vectors.\n",
    "  enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "  target_seq = np.zeros((1, 1))\n",
    "  \n",
    "  # Populate the first character of target sequence with the start character.\n",
    "  # NOTE: tokenizer lower-cases all words\n",
    "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "  # if we get this we break\n",
    "  eos = word2idx_outputs['<eos>']\n",
    "\n",
    "\n",
    "  # [s, c] will be updated in each loop iteration\n",
    "  s = np.zeros((1, LATENT_DIM_DECODER))\n",
    "  c = np.zeros((1, LATENT_DIM_DECODER))\n",
    "\n",
    "\n",
    "  # Create the translation\n",
    "  output_sentence = []\n",
    "  for _ in range(max_len_target):\n",
    "    o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "        \n",
    "\n",
    "    # Get next word\n",
    "    idx = np.argmax(o.flatten())\n",
    "\n",
    "    # End sentence of EOS\n",
    "    if eos == idx:\n",
    "      break\n",
    "\n",
    "    word = ''\n",
    "    if idx > 0:\n",
    "      word = idx2word_trans[idx]\n",
    "      output_sentence.append(word)\n",
    "\n",
    "    # Update the decoder input\n",
    "    # which is just the word just generated\n",
    "    target_seq[0, 0] = idx\n",
    "\n",
    "  return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I agreed.\n",
      "Predicted translation: me senta toalla.\n",
      "Actual translation: Acept. <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: We sat there.\n",
      "Predicted translation: nos sentamos all.\n",
      "Actual translation: Nos sentamos ah. <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: You are good.\n",
      "Predicted translation: eres bueno.\n",
      "Actual translation: Ests bien. <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: We all knew it.\n",
      "Predicted translation: todos lo sabamos.\n",
      "Actual translation: Todos lo sabamos. <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: I need a favor.\n",
      "Predicted translation: necesito una gauchada.\n",
      "Actual translation: Necesito una gauchada. <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: I woke you up.\n",
      "Predicted translation: te he despertado.\n",
      "Actual translation: Te he despertado. <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: Are they tall?\n",
      "Predicted translation: son quera.\n",
      "Actual translation: Son altos? <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: What's up, dude?\n",
      "Predicted translation: qu tengo a yo qu estamos yo qu va\n",
      "Actual translation: Qu pasa, vale? <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: We're trapped!\n",
      "Predicted translation: estamos no reventado.\n",
      "Actual translation: Estamos atrapados! <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: I'm restless.\n",
      "Predicted translation: soy inquieto.\n",
      "Actual translation: Soy intranquilo. <eos>\n",
      "Continue? [Y/n]y\n",
      "-\n",
      "Input sentence: I'm a writer.\n",
      "Predicted translation: soy escritor.\n",
      "Actual translation: Soy escritor. <eos>\n",
      "Continue? [Y/n]n\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "  # Do some test translations\n",
    "  i = np.random.choice(len(input_texts))\n",
    "  input_seq = encoder_inputs[i:i+1]\n",
    "  translation = decode_sequence(input_seq)\n",
    "  print('-')\n",
    "  print('Input sentence:', input_texts[i])\n",
    "  print('Predicted translation:', translation)\n",
    "  print('Actual translation:', target_texts[i])\n",
    "\n",
    "  ans = input(\"Continue? [Y/n]\")\n",
    "  if ans and ans.lower().startswith('n'):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
